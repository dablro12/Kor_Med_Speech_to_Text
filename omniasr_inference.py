import os
import sys
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

import time
import torch
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from tqdm import tqdm

from omnilingual_asr.models.inference.pipeline import ASRInferencePipeline


# --------------------------------------------------------
# 1) Auto Batch Size ê¸°ë°˜ Batch Inference + Timer ì¶”ê°€
# --------------------------------------------------------
def transcribe_audio_batch_auto(batch_paths, pipeline, lang="kor_Hang", init_batch_size=64):
    """
    OmniASRì— ëŒ€í•´ GPU ë©”ëª¨ë¦¬ë¥¼ ë³´ë©´ì„œ ìžë™ìœ¼ë¡œ batch size ì¡°ì ˆí•˜ì—¬ inference.
    OOM ì‹œ batch size ì¤„ì—¬ ìž¬ì‹œë„.
    + inference_time_sec ì¶”ê°€
    """
    batch_langs = [lang] * len(batch_paths)
    batch_size = min(init_batch_size, len(batch_paths))

    while batch_size > 0:
        try:
            # ------------------------------
            # ðŸ”¥ Time measurement ì‹œìž‘
            # ------------------------------
            torch.cuda.synchronize()
            t0 = time.time()

            preds = pipeline.transcribe(
                batch_paths[:batch_size],
                lang=batch_langs[:batch_size],
                batch_size=batch_size
            )

            torch.cuda.synchronize()
            t1 = time.time()
            batch_time = t1 - t0
            per_sample_time = batch_time / batch_size

            return preds, [per_sample_time] * batch_size, batch_size

        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"[WARN] OOM â†’ batch_size {batch_size} -> {batch_size // 2}")
                torch.cuda.empty_cache()
                batch_size = batch_size // 2
            else:
                print("[ERROR] Unexpected runtime error:", e)
                return [""] * batch_size, [0.0] * batch_size, batch_size

    print("[ERROR] Batch failed; returning empty predictions")
    return [""] * len(batch_paths), [0.0] * len(batch_paths), 1



# --------------------------------------------------------
# 2) Auto Batch Inference + Parquet ì €ìž¥ + Resume
# --------------------------------------------------------
def run_inference_omni(model_card, csv_path, save_path=None, lang="kor_Hang", init_batch_size=64):

    pipeline = ASRInferencePipeline(model_card=model_card)
    df = pd.read_csv(csv_path)
    parquet_path = save_path.replace(".csv", ".parquet")

    # --------------------------------------------------------
    # Result dict (ðŸ”¥ inference_time_sec ì¶”ê°€)
    # --------------------------------------------------------
    results = {
        "abs_path": [],
        "gt_text": [],
        "pred_text": [],
        "inference_time_sec": []
    }

    # --------------------------------------------------------
    # ë””ë ‰í† ë¦¬ ìƒì„±
    # --------------------------------------------------------
    save_dir = os.path.dirname(parquet_path)
    if save_dir and not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # --------------------------------------------------------
    # Resume
    # --------------------------------------------------------
    if os.path.exists(parquet_path):
        prev_df = pq.read_table(parquet_path).to_pandas()

        results["abs_path"].extend(prev_df["abs_path"].tolist())
        results["gt_text"].extend(prev_df["gt_text"].tolist())
        results["pred_text"].extend(prev_df["pred_text"].tolist())

        # ðŸ”¥ Resume ì‹œ inference_time_sec ì»¬ëŸ¼ ìœ ì§€
        if "inference_time_sec" in prev_df.columns:
            results["inference_time_sec"].extend(prev_df["inference_time_sec"].tolist())
        else:
            results["inference_time_sec"].extend([0.0] * len(prev_df))

        done = set(prev_df["abs_path"].tolist())
        print(f"[INFO] Resumed from: {parquet_path} (already {len(done)} rows)")
    else:
        done = set()

    batch_paths = []
    batch_gt = []
    processed_count = 0

    print(f"[INFO] Total rows={len(df)}, Already done={len(done)}")

    # --------------------------------------------------------
    # Main inference loop
    # --------------------------------------------------------
    for _, row in tqdm(df.iterrows(), total=len(df), desc=f"Inference: {model_card}"):

        audio_path = row["abs_path"]
        gt_text = row["transcription"]

        if audio_path in done:
            continue

        batch_paths.append(audio_path)
        batch_gt.append(gt_text)

        if len(batch_paths) >= init_batch_size:

            preds, times, used_bs = transcribe_audio_batch_auto(
                batch_paths, pipeline, lang=lang, init_batch_size=init_batch_size
            )

            # ê²°ê³¼ ì €ìž¥
            results["abs_path"].extend(batch_paths[:used_bs])
            results["gt_text"].extend(batch_gt[:used_bs])
            results["pred_text"].extend(preds)
            results["inference_time_sec"].extend(times)

            batch_paths = batch_paths[used_bs:]
            batch_gt = batch_gt[used_bs:]
            processed_count += used_bs

            # ì£¼ê¸°ì  ì €ìž¥
            if processed_count % 10000 == 0:
                print(f"[INFO] Saving chunk â†’ {parquet_path}")
                table = pa.Table.from_pandas(pd.DataFrame(results))
                pq.write_table(table, parquet_path)

    # --------------------------------------------------------
    # Final leftover ì²˜ë¦¬
    # --------------------------------------------------------
    if len(batch_paths) > 0:
        preds, times, used_bs = transcribe_audio_batch_auto(
            batch_paths, pipeline, lang=lang, init_batch_size=init_batch_size
        )

        results["abs_path"].extend(batch_paths[:used_bs])
        results["gt_text"].extend(batch_gt[:used_bs])
        results["pred_text"].extend(preds)
        results["inference_time_sec"].extend(times)

    # --------------------------------------------------------
    # Final save
    # --------------------------------------------------------
    print(f"[INFO] Final save â†’ {parquet_path}")
    table = pa.Table.from_pandas(pd.DataFrame(results))
    pq.write_table(table, parquet_path)

    return pd.DataFrame(results)



# --------------------------------------------------------
# 3) ì‹¤í–‰ ì½”ë“œ
# --------------------------------------------------------
if __name__ == "__main__":
    model_series = [
        'omniASR_CTC_300M',
        'omniASR_CTC_1B',
        'omniASR_CTC_3B',
        'omniASR_CTC_7B'
    ]

    for model_card in model_series:
        run_inference_omni(
            model_card=model_card,
            csv_path="/workspace/kru_data/test.csv",
            save_path=f"/workspace/results/omniasr_inference/omniasr_ctc/{model_card}/test_pred.csv",
            lang="kor_Hang",
            init_batch_size=64
        )
