{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e14d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper-base 예측 결과:  근데 행동치료 감각통업치고 가는 게 발달이 돼서 이런 치료를 하면 굉장히 훨씬 나아질 수가 있거든요.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 오디오 파일 경로\n",
    "snu_data_sample_path = \"/workspace/kor_med_stt_data/snu_data/강지예_201812001_행동치료.wav\"\n",
    "kru_data_sample_path = \"/workspace/kru_data/원천데이터/의사/HB_0276/HB_0276-1004-01-01-M-04-A.wav\"\n",
    "\n",
    "# Whisper-base 모델과 프로세서 로드 (한국어)\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"ko\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 오디오 로드\n",
    "audio, sr = librosa.load(\n",
    "    snu_data_sample_path\n",
    "    # kru_data_sample_path\n",
    "    , sr=16000)\n",
    "\n",
    "# 입력 특성 추출\n",
    "input_features = processor.feature_extractor(\n",
    "    audio, sampling_rate=16000, return_tensors=\"pt\"\n",
    ").input_features.to(model.device)\n",
    "\n",
    "# 예측\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Whisper-base 예측 결과:\", transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f45e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor 로드: openai/whisper-base\n",
      "Model 로드: /workspace/results/whisper_train/whisper-base/checkpoint-5090605\n",
      "오디오 입력: /workspace/kor_med_stt_data/snu_data/강지예_201812001_행동치료.wav\n",
      "오디오 shape: (125675,), dtype: float32\n",
      "input_features shape: torch.Size([1, 80, 3000])\n",
      "디코딩 결과: 생활이��니다. 이런 치료를 서명할 수 있는 때까지 나아질 수 있거든요.\n",
      "Custom checkpoint 예측 결과: 생활이��니다. 이런 치료를 서명할 수 있는 때까지 나아질 수 있거든요.\n",
      "✅ 결과가 정상적으로 출력되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 모델 체크포인트 디렉토리\n",
    "custom_ckpt_dir = \"/workspace/results/whisper_train/whisper-base/checkpoint-5090605\"\n",
    "\n",
    "# WhisperInference 클래스 정의 참고(위의 whisper_tester.py 기반)\n",
    "class WhisperInference:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dir: str,\n",
    "        sampling_rate: int = 16000,\n",
    "        device: str = \"cuda\",\n",
    "        init_batch_size: int = 64\n",
    "    ):\n",
    "        self.model_dir = model_dir\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.device = device\n",
    "        self.init_batch_size = init_batch_size\n",
    "\n",
    "        model_name = os.path.basename(os.path.dirname(self.model_dir))\n",
    "        hf_model_id = f\"openai/{model_name}\"\n",
    "\n",
    "        # 확실하게 processor/model이 문제가 없는지 로그 찍기\n",
    "        print(f\"Processor 로드: {hf_model_id}\")\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            hf_model_id, language=\"ko\", task=\"transcribe\"\n",
    "        )\n",
    "        print(f\"Model 로드: {model_dir}\")\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_dir).to(device)\n",
    "\n",
    "    def transcribe(self, audio_path):\n",
    "        print(f\"오디오 입력: {audio_path}\")\n",
    "        # 오디오 로드\n",
    "        try:\n",
    "            audio, _ = librosa.load(audio_path, sr=self.sampling_rate)\n",
    "            print(f\"오디오 shape: {audio.shape}, dtype: {audio.dtype}\")\n",
    "        except Exception as e:\n",
    "            print(\"오디오 로드 오류:\", e)\n",
    "            audio = np.zeros(self.sampling_rate, dtype=np.float32)\n",
    "            print(\"0으로 대체\")\n",
    "\n",
    "        feats = self.processor.feature_extractor(\n",
    "            audio, sampling_rate=self.sampling_rate, return_tensors=\"pt\"\n",
    "        ).input_features.to(self.device)\n",
    "        print(f\"input_features shape: {feats.shape}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_ids = self.model.generate(feats)\n",
    "        text = self.processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "        print(\"디코딩 결과:\", text)\n",
    "        return text\n",
    "\n",
    "whisper_infer = WhisperInference(\n",
    "    model_dir=custom_ckpt_dir,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "custom_ckpt_transcription = whisper_infer.transcribe(\n",
    "    snu_data_sample_path\n",
    "    # kru_data_sample_path\n",
    "    )\n",
    "\n",
    "print(\"Custom checkpoint 예측 결과:\", custom_ckpt_transcription)\n",
    "\n",
    "if not custom_ckpt_transcription or custom_ckpt_transcription.strip() == \"\":\n",
    "    print(\"⚠️ 결과가 비어 있습니다. 모델이나 체크포인트, 입력 오디오 경로 등을 다시 확인하십시오.\")\n",
    "else:\n",
    "    print(\"✅ 결과가 정상적으로 출력되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f1614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
