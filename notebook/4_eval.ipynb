{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb8d694",
   "metadata": {},
   "source": [
    "### í•™ìŠµ ëª¨ë¸ Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    WhisperForConditionalGeneration, WhisperProcessor\n",
    ")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-tiny\"\n",
    "checkpoint_dir = f\"/workspace/results/whisper_train/{model_id.split('/')[-1]}/ -10000\"\n",
    "lang = \"ko\"\n",
    "task = \"transcribe\"\n",
    "\n",
    "test_df = pd.read_csv(\"/workspace/kru_data/test.csv\")\n",
    "sample_file = test_df.iloc[0][\"abs_path\"]\n",
    "# sample_file = \"/workspace/kor_med_stt_data/snu_data/ë§ì´ì¢€ëŠ¦ëŠ”ê²ƒê°™ì•„ìš”_ìƒ˜í”Œë°ì´í„°ì…‹/ë‚¨í•™ìƒì˜ì‚¬_ì—¬í™˜ì_1.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647abd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "waveform, sr = librosa.load(sample_file, sr=16000)\n",
    "# ì²˜ìŒ 5ì´ˆëŠ” ë„˜ì–´ê°€ê³  5~35ì´ˆ ê°€ì§€ê³ ì˜¤ê¸°\n",
    "start_sec = 0\n",
    "end_sec = 3\n",
    "start_sample = start_sec * sr\n",
    "end_sample = end_sec * sr\n",
    "if len(waveform) > end_sample:\n",
    "    print(f\"This file length is {len(waveform)/sr} seconds. Using audio from {start_sec} to {end_sec} seconds.\")\n",
    "    waveform = waveform[start_sample:end_sample]\n",
    "else:\n",
    "    print(f\"This file length is {len(waveform)/sr} seconds. Using available audio from {start_sec} seconds to end.\")\n",
    "    waveform = waveform[start_sample:]\n",
    "    \n",
    "# samplingëœ ì˜¤ë””ì˜¤ jupyter ì—ì„œ ë“£ê¸°\n",
    "import IPython.display as ipd\n",
    "ipd.Audio(waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\n",
    "    model_id, language=\"ko\", task=\"transcribe\"\n",
    ")\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(checkpoint_dir).to(\"cuda\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9eff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_long_audio(processor, model, waveform, sr, chunk_sec=5):\n",
    "    chunk_size = chunk_sec * sr\n",
    "    texts = []\n",
    "    last_text = \"\"\n",
    "\n",
    "    for start in range(0, len(waveform), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk = waveform[start:end]\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "\n",
    "        # Whisper inference\n",
    "        input_features = processor.feature_extractor(\n",
    "            chunk,\n",
    "            sampling_rate=sr,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_ids = model.generate(input_features)\n",
    "\n",
    "        text = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        # ğŸ”¥ ì¤‘ë³µ ì œê±°\n",
    "        if last_text:\n",
    "            # ë§ˆì§€ë§‰ 15ì ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ê²€ì‚¬\n",
    "            overlap = last_text[-15:]\n",
    "            if text.startswith(overlap):\n",
    "                text = text[len(overlap):]\n",
    "\n",
    "        texts.append(text)\n",
    "        last_text = text\n",
    "\n",
    "    return \" \".join(texts).strip()\n",
    "\n",
    "full_text = transcribe_long_audio(\n",
    "    processor,\n",
    "    model,\n",
    "    waveform,\n",
    "    sr,\n",
    "    chunk_sec=3,     # ì—¬ê¸°ì„œ ì›í•˜ëŠ” ê¸¸ì´ ì„¤ì •\n",
    ")\n",
    "\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d8164",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ffe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval import fast_asr_metrics\n",
    "import json \n",
    "import pandas as pd\n",
    "\n",
    "# omniasr inference model\n",
    "# file_path = \"/workspace/results/omniasr_inference/omniasr_ctc/omniASR_CTC_300M/test_pred.parquet\" # omni asr \n",
    "\n",
    "# whisper train model \n",
    "file_path = \"/workspace/results/whisper_test/whisper-tiny/checkpoint-10000/test_pred.parquet\" # whisper train model\n",
    "# whisper inference model\n",
    "# file_path = \"/workspace/results/whisper_inference/whisper_tiny_inference/test_pred.parquet\" # whisper inference model\n",
    "\n",
    "save_path = file_path.replace(file_path.split(\"/\")[-1], \"metrics.json\") # file_path ì˜ ë””ë ‰í† ë¦¬\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "metrics = fast_asr_metrics(df, gt_col = \"gt_text\", pred_col = \"pred_text\")\n",
    "print(metrics)\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "    print(f\"Metrics saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89394402",
   "metadata": {},
   "source": [
    "### ì „ì²´ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d2788ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1018124\n",
      "{'wer_all': 0.7354223130933701, 'cer_all': 0.2882213275523898, 'wer_mean': 0.7236693159795979, 'wer_std': 6.723327342917463, 'wer_median': 0.0, 'wer_min': 0.0, 'wer_max': 1800.0, 'cer_mean': 0.2840517805139876, 'cer_std': 6.440914913864843, 'cer_median': 0.0, 'cer_min': 0.0, 'cer_max': 2950.0, 'inference_time_sec_mean': 0.23499111199137507, 'inference_time_sec_std': 0.05678054619251264, 'inference_time_sec_median': 0.21762687961260477, 'inference_time_sec_min': 0.1138710230588913, 'inference_time_sec_max': 0.4552684525648753, 'num_samples': 254457, 'model_name': 'checkpoint-1018124'}\n",
      "Metrics dataframe saved to /workspace/kor_med_stt_data/results/whisper_test/whisper-medium/checkpoint-1018124/metrics.df\n",
      "checkpoint-1272655\n",
      "{'wer_all': 0.6292702151435527, 'cer_all': 0.2323410566994393, 'wer_mean': 0.6170903188904378, 'wer_std': 6.311219512490857, 'wer_median': 0.0, 'wer_min': 0.0, 'wer_max': 1800.0, 'cer_mean': 0.23075193981146724, 'cer_std': 6.2792593730377915, 'cer_median': 0.0, 'cer_min': 0.0, 'cer_max': 2950.0, 'inference_time_sec_mean': 0.18137607341146347, 'inference_time_sec_std': 0.024163922862115324, 'inference_time_sec_median': 0.18904843926429749, 'inference_time_sec_min': 0.13511582215627035, 'inference_time_sec_max': 0.29594047864278156, 'num_samples': 254445, 'model_name': 'checkpoint-1272655'}\n",
      "Metrics dataframe saved to /workspace/kor_med_stt_data/results/whisper_test/whisper-medium/checkpoint-1272655/metrics.df\n",
      "checkpoint-254531\n",
      "{'wer_all': 2.1285962029083034, 'cer_all': 0.7914199364694183, 'wer_mean': 2.086437740779527, 'wer_std': 9.854139636942135, 'wer_median': 0.0, 'wer_min': 0.0, 'wer_max': 1800.0, 'cer_mean': 0.7568549941606583, 'cer_std': 7.763704750099207, 'cer_median': 0.0, 'cer_min': 0.0, 'cer_max': 2950.0, 'inference_time_sec_mean': 0.17638542039650792, 'inference_time_sec_std': 0.039587297446612525, 'inference_time_sec_median': 0.1541330615679423, 'inference_time_sec_min': 0.13384661078453064, 'inference_time_sec_max': 0.4875751535097758, 'num_samples': 254445, 'model_name': 'checkpoint-254531'}\n",
      "Metrics dataframe saved to /workspace/kor_med_stt_data/results/whisper_test/whisper-medium/checkpoint-254531/metrics.df\n",
      "checkpoint-509062\n",
      "{'wer_all': 1.298167018142493, 'cer_all': 0.5327251195820639, 'wer_mean': 1.2648959589632283, 'wer_std': 8.505677253607052, 'wer_median': 0.0, 'wer_min': 0.0, 'wer_max': 1800.0, 'cer_mean': 0.48813137349213864, 'cer_std': 6.9615528783331575, 'cer_median': 0.0, 'cer_min': 0.0, 'cer_max': 2950.0, 'inference_time_sec_mean': 0.21153367527783676, 'inference_time_sec_std': 0.03568439920921693, 'inference_time_sec_median': 0.20476867258548737, 'inference_time_sec_min': 0.15240445236365, 'inference_time_sec_max': 0.7776462684075037, 'num_samples': 254445, 'model_name': 'checkpoint-509062'}\n",
      "Metrics dataframe saved to /workspace/kor_med_stt_data/results/whisper_test/whisper-medium/checkpoint-509062/metrics.df\n",
      "checkpoint-763593\n",
      "{'wer_all': 0.9658800668896803, 'cer_all': 0.3896564869949637, 'wer_mean': 0.9675386992143746, 'wer_std': 10.912287491807293, 'wer_median': 0.0, 'wer_min': 0.0, 'wer_max': 1800.0, 'cer_mean': 0.3893312942471978, 'cer_std': 8.665026630660616, 'cer_median': 0.0, 'cer_min': 0.0, 'cer_max': 2950.0, 'inference_time_sec_mean': 0.17705108894578725, 'inference_time_sec_std': 0.01110304488321668, 'inference_time_sec_median': 0.17546631892522177, 'inference_time_sec_min': 0.15324104825655618, 'inference_time_sec_max': 0.27339256306489307, 'num_samples': 254445, 'model_name': 'checkpoint-763593'}\n",
      "Metrics dataframe saved to /workspace/kor_med_stt_data/results/whisper_test/whisper-medium/checkpoint-763593/metrics.df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>wer_all</th>\n",
       "      <th>cer_all</th>\n",
       "      <th>wer_mean</th>\n",
       "      <th>wer_std</th>\n",
       "      <th>wer_median</th>\n",
       "      <th>wer_min</th>\n",
       "      <th>wer_max</th>\n",
       "      <th>cer_mean</th>\n",
       "      <th>cer_std</th>\n",
       "      <th>cer_median</th>\n",
       "      <th>cer_min</th>\n",
       "      <th>cer_max</th>\n",
       "      <th>inference_time_sec_mean</th>\n",
       "      <th>inference_time_sec_std</th>\n",
       "      <th>inference_time_sec_median</th>\n",
       "      <th>inference_time_sec_min</th>\n",
       "      <th>inference_time_sec_max</th>\n",
       "      <th>num_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>checkpoint-1272655</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.617</td>\n",
       "      <td>6.311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>0.231</td>\n",
       "      <td>6.279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.296</td>\n",
       "      <td>254445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>checkpoint-1018124</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.724</td>\n",
       "      <td>6.723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>0.284</td>\n",
       "      <td>6.441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.455</td>\n",
       "      <td>254457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>checkpoint-763593</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.968</td>\n",
       "      <td>10.912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>0.389</td>\n",
       "      <td>8.665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.273</td>\n",
       "      <td>254445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>checkpoint-509062</td>\n",
       "      <td>1.298</td>\n",
       "      <td>0.533</td>\n",
       "      <td>1.265</td>\n",
       "      <td>8.506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>0.488</td>\n",
       "      <td>6.962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.778</td>\n",
       "      <td>254445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>checkpoint-254531</td>\n",
       "      <td>2.129</td>\n",
       "      <td>0.791</td>\n",
       "      <td>2.086</td>\n",
       "      <td>9.854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>0.757</td>\n",
       "      <td>7.764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.488</td>\n",
       "      <td>254445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name  wer_all  cer_all  wer_mean  wer_std  wer_median  \\\n",
       "1  checkpoint-1272655    0.629    0.232     0.617    6.311         0.0   \n",
       "0  checkpoint-1018124    0.735    0.288     0.724    6.723         0.0   \n",
       "4   checkpoint-763593    0.966    0.390     0.968   10.912         0.0   \n",
       "3   checkpoint-509062    1.298    0.533     1.265    8.506         0.0   \n",
       "2   checkpoint-254531    2.129    0.791     2.086    9.854         0.0   \n",
       "\n",
       "   wer_min  wer_max  cer_mean  cer_std  cer_median  cer_min  cer_max  \\\n",
       "1      0.0   1800.0     0.231    6.279         0.0      0.0   2950.0   \n",
       "0      0.0   1800.0     0.284    6.441         0.0      0.0   2950.0   \n",
       "4      0.0   1800.0     0.389    8.665         0.0      0.0   2950.0   \n",
       "3      0.0   1800.0     0.488    6.962         0.0      0.0   2950.0   \n",
       "2      0.0   1800.0     0.757    7.764         0.0      0.0   2950.0   \n",
       "\n",
       "   inference_time_sec_mean  inference_time_sec_std  inference_time_sec_median  \\\n",
       "1                    0.181                   0.024                      0.189   \n",
       "0                    0.235                   0.057                      0.218   \n",
       "4                    0.177                   0.011                      0.175   \n",
       "3                    0.212                   0.036                      0.205   \n",
       "2                    0.176                   0.040                      0.154   \n",
       "\n",
       "   inference_time_sec_min  inference_time_sec_max  num_samples  \n",
       "1                   0.135                   0.296       254445  \n",
       "0                   0.114                   0.455       254457  \n",
       "4                   0.153                   0.273       254445  \n",
       "3                   0.152                   0.778       254445  \n",
       "2                   0.134                   0.488       254445  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.eval import fast_asr_metrics\n",
    "import json \n",
    "import pandas as pd\n",
    "from glob import glob \n",
    "# test_root_dir = \"/workspace/results/\"\n",
    "test_root_dir = \"/workspace/kor_med_stt_data/results/whisper_test/whisper-medium\"\n",
    "\n",
    "# test_root_dir ë‚´ ëª¨ë“  test_pred.parquet íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ ì°¾ê¸°\n",
    "test_files = sorted(glob(f\"{test_root_dir}/**/test_pred.parquet\", recursive=True))\n",
    "all_list = []\n",
    "for file_path in test_files:\n",
    "    save_path = file_path.replace(file_path.split(\"/\")[-1], \"metrics.df\") # file_path ì˜ ë””ë ‰í† ë¦¬\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    metrics = fast_asr_metrics(df, gt_col = \"gt_text\", pred_col = \"pred_text\")\n",
    "    metrics['model_name'] = file_path.split(\"/\")[-2]\n",
    "    print(file_path.split(\"/\")[-2])\n",
    "    print(metrics)\n",
    "\n",
    "    # metricsë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ í›„ csvë¡œ ì €ì¥\n",
    "    \n",
    "    df_metrics = pd.DataFrame([metrics])\n",
    "    df_metrics = df_metrics.sort_values(by='wer_mean', ascending=True)\n",
    "    df_metrics.to_csv(save_path, index=False)\n",
    "    print(f\"Metrics dataframe saved to {save_path}\")\n",
    "    \n",
    "    all_list.append(metrics)\n",
    "\n",
    "all_df = pd.DataFrame(all_list)\n",
    "# 'model_name' ì»¬ëŸ¼ì´ ê°€ì¥ ì²« ë²ˆì§¸ ì»¬ëŸ¼ìœ¼ë¡œ ì˜¤ë„ë¡ ìˆœì„œ ì¡°ì •\n",
    "cols = ['model_name'] + [col for col in all_df.columns if col != 'model_name']\n",
    "all_df = all_df[cols]\n",
    "all_df = all_df.sort_values(by='wer_mean', ascending=True)\n",
    "all_df.to_csv(f\"{test_root_dir}/all_metrics.csv\", index=False)\n",
    "all_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e04355a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>CER â†“ (mean Â± std)</th>\n",
       "      <th>WER â†“ (mean Â± std)</th>\n",
       "      <th>Inference Time â†“(mean Â± std, Sec)</th>\n",
       "      <th>Training Time(Hours:Minutes:Seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.284 Â± 6.441</td>\n",
       "      <td>0.724 Â± 6.723</td>\n",
       "      <td>0.235 Â± 0.057</td>\n",
       "      <td>282:48:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.231 Â± 6.279</td>\n",
       "      <td>0.617 Â± 6.311</td>\n",
       "      <td>0.181 Â± 0.024</td>\n",
       "      <td>353:30:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.757 Â± 7.764</td>\n",
       "      <td>2.086 Â± 9.854</td>\n",
       "      <td>0.176 Â± 0.04</td>\n",
       "      <td>70:42:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.488 Â± 6.962</td>\n",
       "      <td>1.265 Â± 8.506</td>\n",
       "      <td>0.212 Â± 0.036</td>\n",
       "      <td>141:24:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.389 Â± 8.665</td>\n",
       "      <td>0.968 Â± 10.912</td>\n",
       "      <td>0.177 Â± 0.011</td>\n",
       "      <td>212:06:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch CER â†“ (mean Â± std) WER â†“ (mean Â± std)  \\\n",
       "0      1      0.284 Â± 6.441      0.724 Â± 6.723   \n",
       "1      2      0.231 Â± 6.279      0.617 Â± 6.311   \n",
       "2      3      0.757 Â± 7.764      2.086 Â± 9.854   \n",
       "3      4      0.488 Â± 6.962      1.265 Â± 8.506   \n",
       "4      5      0.389 Â± 8.665     0.968 Â± 10.912   \n",
       "\n",
       "  Inference Time â†“(mean Â± std, Sec) Training Time(Hours:Minutes:Seconds)  \n",
       "0                     0.235 Â± 0.057                            282:48:44  \n",
       "1                     0.181 Â± 0.024                            353:30:55  \n",
       "2                      0.176 Â± 0.04                             70:42:11  \n",
       "3                     0.212 Â± 0.036                            141:24:22  \n",
       "4                     0.177 Â± 0.011                            212:06:33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = all_df.sort_values(by='model_name', ascending=True)\n",
    "\n",
    "import datetime\n",
    "\n",
    "def seconds_to_h_m_s(seconds):\n",
    "    # Converts seconds to HH:MM:SS, never shows days!\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds_left = seconds % 60\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds_left:02}\"\n",
    "\n",
    "# Extract training time in seconds and convert to HH:MM:SS format (never with days)\n",
    "training_seconds = [int(name.split(\"-\")[-1]) for name in all_df[\"model_name\"]]\n",
    "training_hms = [seconds_to_h_m_s(sec) for sec in training_seconds]\n",
    "\n",
    "# ì»¬ëŸ¼ ì¬ì •ì˜ ë° ë°ì´í„° ìƒì„±\n",
    "display_df = pd.DataFrame({\n",
    "    \"Epoch\": range(1, len(all_df)+1),\n",
    "    \"CER â†“ (mean Â± std)\": all_df[\"cer_mean\"].round(3).astype(str) + \" Â± \" + all_df[\"cer_std\"].round(3).astype(str),\n",
    "    \"WER â†“ (mean Â± std)\": all_df[\"wer_mean\"].round(3).astype(str) + \" Â± \" + all_df[\"wer_std\"].round(3).astype(str),\n",
    "    \"Inference Time â†“(mean Â± std, Sec)\": all_df[\"inference_time_sec_mean\"].round(3).astype(str) + \" Â± \" + all_df[\"inference_time_sec_std\"].round(3).astype(str),\n",
    "    \"Training Time(Hours:Minutes:Seconds)\": training_hms\n",
    "})\n",
    "display_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def hh_mm_ss_to_seconds(time_str):\n",
    "    # Split off microseconds if present\n",
    "    if '.' in time_str:\n",
    "        time_str, micro = time_str.split('.')\n",
    "    else:\n",
    "        micro = '0'\n",
    "    t = datetime.datetime.strptime(time_str, \"%H:%M:%S\")\n",
    "    delta = datetime.timedelta(\n",
    "        hours=t.hour, minutes=t.minute, seconds=t.second, microseconds=int(micro)\n",
    "    )\n",
    "    return delta.total_seconds()\n",
    "\n",
    "def second_to_hh_mm_ss(seconds):\n",
    "    return datetime.timedelta(seconds=seconds)\n",
    "\n",
    "second = hh_mm_ss_to_seconds(\"10:49:38.484706\")\n",
    "print(second)\n",
    "\n",
    "hh_mm_ss = second_to_hh_mm_ss(second * 1 / 5)\n",
    "print(f\"{hh_mm_ss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54051133",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"/workspace/results/omniasr_inference/omniasr_ctc/omniASR_CTC_1B/test_pred.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ef5d160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>CER â†“(mean Â± std)</th>\n",
       "      <th>WER â†“(mean Â± std)</th>\n",
       "      <th>Inference Time â†“(mean Â± std, Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omniASR_CTC_300M</td>\n",
       "      <td>26.572 Â± 25.709</td>\n",
       "      <td>69.229 Â± 28.801</td>\n",
       "      <td>0.073 Â± 0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omniASR_CTC_1B</td>\n",
       "      <td>20.513 Â± 20.685</td>\n",
       "      <td>59.631 Â± 29.609</td>\n",
       "      <td>0.187 Â± 0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omniASR_CTC_3B</td>\n",
       "      <td>20.461 Â± 23.579</td>\n",
       "      <td>57.483 Â± 30.922</td>\n",
       "      <td>0.456 Â± 0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omniASR_CTC_7B</td>\n",
       "      <td>19.957 Â± 23.730</td>\n",
       "      <td>56.609 Â± 30.625</td>\n",
       "      <td>0.745 Â± 0.260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model_name CER â†“(mean Â± std) WER â†“(mean Â± std)  \\\n",
       "0  omniASR_CTC_300M   26.572 Â± 25.709   69.229 Â± 28.801   \n",
       "0    omniASR_CTC_1B   20.513 Â± 20.685   59.631 Â± 29.609   \n",
       "0    omniASR_CTC_3B   20.461 Â± 23.579   57.483 Â± 30.922   \n",
       "0    omniASR_CTC_7B   19.957 Â± 23.730   56.609 Â± 30.625   \n",
       "\n",
       "  Inference Time â†“(mean Â± std, Sec)  \n",
       "0                     0.073 Â± 0.047  \n",
       "0                     0.187 Â± 0.068  \n",
       "0                     0.456 Â± 0.154  \n",
       "0                     0.745 Â± 0.260  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "result_files = glob(\"/workspace/results/omniasr_inference/omniasr_ctc/**/metrics.df\")\n",
    "\n",
    "all_list = []\n",
    "for result_file in result_files:\n",
    "    model_name = result_file.split(\"/\")[-2]\n",
    "    df = pd.read_csv(result_file)\n",
    "    df['model_name'] = model_name\n",
    "    all_list.append(df)\n",
    "\n",
    "# model_nameì´ ë§¨ì• ì—´ì— ì˜¤ë„ë¡ ê° DataFrameì˜ ì—´ ìˆœì„œ ì¡°ì •\n",
    "for i, df in enumerate(all_list):\n",
    "    if 'model_name' in df.columns:\n",
    "        cols = df.columns.tolist()\n",
    "        cols.insert(0, cols.pop(cols.index('model_name')))\n",
    "        all_list[i] = df[cols]\n",
    "\n",
    "all_df = pd.concat(all_list)\n",
    "all_df = all_df.sort_values(by='wer_mean', ascending=True)\n",
    "# ì›í•˜ëŠ” ì¹¼ëŸ¼: model_name, CER â†“(mean Â± std), WER â†“(mean Â± std), Inference Time â†“(mean Â± std, Sec)\n",
    "\n",
    "def format_pm(mean, std):\n",
    "    # float: ì†Œìˆ˜ì  ì…‹ì§¸ ìë¦¬ê¹Œì§€ í‘œì‹œ\n",
    "    return f\"{mean:.3f} Â± {std:.3f}\"\n",
    "\n",
    "# ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„±\n",
    "all_df['CER â†“(mean Â± std)'] = all_df.apply(lambda x: format_pm(x['cer_mean'], x['cer_std']), axis=1)\n",
    "all_df['WER â†“(mean Â± std)'] = all_df.apply(lambda x: format_pm(x['wer_mean'], x['wer_std']), axis=1)\n",
    "if 'inference_time_sec_mean' in all_df.columns and 'inference_time_sec_std' in all_df.columns:\n",
    "    all_df['Inference Time â†“(mean Â± std, Sec)'] = all_df.apply(\n",
    "        lambda x: format_pm(x['inference_time_sec_mean'], x['inference_time_sec_std']), axis=1\n",
    "    )\n",
    "else:\n",
    "    all_df['Inference Time â†“(mean Â± std, Sec)'] = \"-\"\n",
    "\n",
    "# ë³´ì—¬ì¤„ ì»¬ëŸ¼ ìˆœì„œ ì§€ì •\n",
    "final_cols = [\n",
    "    'model_name',\n",
    "    'CER â†“(mean Â± std)',\n",
    "    'WER â†“(mean Â± std)',\n",
    "    'Inference Time â†“(mean Â± std, Sec)'\n",
    "]\n",
    "\n",
    "all_df[final_cols].sort_values(by='WER â†“(mean Â± std)', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
