{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb8d694",
   "metadata": {},
   "source": [
    "### ÌïôÏäµ Î™®Îç∏ Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    WhisperForConditionalGeneration, WhisperProcessor\n",
    ")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-tiny\"\n",
    "checkpoint_dir = f\"/workspace/results/whisper_train/{model_id.split('/')[-1]}/checkpoint-10000\"\n",
    "lang = \"ko\"\n",
    "task = \"transcribe\"\n",
    "\n",
    "test_df = pd.read_csv(\"/workspace/kru_data/test.csv\")\n",
    "sample_file = test_df.iloc[0][\"abs_path\"]\n",
    "# sample_file = \"/workspace/kor_med_stt_data/snu_data/ÎßêÏù¥Ï¢ÄÎä¶ÎäîÍ≤ÉÍ∞ôÏïÑÏöî_ÏÉòÌîåÎç∞Ïù¥ÌÑ∞ÏÖã/ÎÇ®ÌïôÏÉùÏùòÏÇ¨_Ïó¨ÌôòÏûê_1.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647abd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "waveform, sr = librosa.load(sample_file, sr=16000)\n",
    "# Ï≤òÏùå 5Ï¥àÎäî ÎÑòÏñ¥Í∞ÄÍ≥† 5~35Ï¥à Í∞ÄÏßÄÍ≥†Ïò§Í∏∞\n",
    "start_sec = 0\n",
    "end_sec = 3\n",
    "start_sample = start_sec * sr\n",
    "end_sample = end_sec * sr\n",
    "if len(waveform) > end_sample:\n",
    "    print(f\"This file length is {len(waveform)/sr} seconds. Using audio from {start_sec} to {end_sec} seconds.\")\n",
    "    waveform = waveform[start_sample:end_sample]\n",
    "else:\n",
    "    print(f\"This file length is {len(waveform)/sr} seconds. Using available audio from {start_sec} seconds to end.\")\n",
    "    waveform = waveform[start_sample:]\n",
    "    \n",
    "# samplingÎêú Ïò§ÎîîÏò§ jupyter ÏóêÏÑú Îì£Í∏∞\n",
    "import IPython.display as ipd\n",
    "ipd.Audio(waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\n",
    "    model_id, language=\"ko\", task=\"transcribe\"\n",
    ")\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(checkpoint_dir).to(\"cuda\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9eff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_long_audio(processor, model, waveform, sr, chunk_sec=5):\n",
    "    chunk_size = chunk_sec * sr\n",
    "    texts = []\n",
    "    last_text = \"\"\n",
    "\n",
    "    for start in range(0, len(waveform), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk = waveform[start:end]\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "\n",
    "        # Whisper inference\n",
    "        input_features = processor.feature_extractor(\n",
    "            chunk,\n",
    "            sampling_rate=sr,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_ids = model.generate(input_features)\n",
    "\n",
    "        text = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        # üî• Ï§ëÎ≥µ Ï†úÍ±∞\n",
    "        if last_text:\n",
    "            # ÎßàÏßÄÎßâ 15Ïûê Í∏∞Ï§ÄÏúºÎ°ú Ï§ëÎ≥µ Í≤ÄÏÇ¨\n",
    "            overlap = last_text[-15:]\n",
    "            if text.startswith(overlap):\n",
    "                text = text[len(overlap):]\n",
    "\n",
    "        texts.append(text)\n",
    "        last_text = text\n",
    "\n",
    "    return \" \".join(texts).strip()\n",
    "\n",
    "full_text = transcribe_long_audio(\n",
    "    processor,\n",
    "    model,\n",
    "    waveform,\n",
    "    sr,\n",
    "    chunk_sec=3,     # Ïó¨Í∏∞ÏÑú ÏõêÌïòÎäî Í∏∏Ïù¥ ÏÑ§Ï†ï\n",
    ")\n",
    "\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d8164",
   "metadata": {},
   "source": [
    "## Î™®Îç∏ ÌèâÍ∞Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ffe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval import fast_asr_metrics\n",
    "import json \n",
    "import pandas as pd\n",
    "\n",
    "# omniasr inference model\n",
    "# file_path = \"/workspace/results/omniasr_inference/omniasr_ctc/omniASR_CTC_300M/test_pred.parquet\" # omni asr \n",
    "\n",
    "# whisper train model \n",
    "file_path = \"/workspace/results/whisper_test/whisper-tiny/checkpoint-10000/test_pred.parquet\" # whisper train model\n",
    "# whisper inference model\n",
    "# file_path = \"/workspace/results/whisper_inference/whisper_tiny_inference/test_pred.parquet\" # whisper inference model\n",
    "\n",
    "save_path = file_path.replace(file_path.split(\"/\")[-1], \"metrics.json\") # file_path Ïùò ÎîîÎ†âÌÜ†Î¶¨\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "metrics = fast_asr_metrics(df, gt_col = \"gt_text\", pred_col = \"pred_text\")\n",
    "print(metrics)\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "    print(f\"Metrics saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89394402",
   "metadata": {},
   "source": [
    "### Ï†ÑÏ≤¥ ÌèâÍ∞Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2788ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omniASR_CTC_1B\n",
      "{'wer_all': 54.651519604453355, 'cer_all': 18.163502070508773, 'wer_mean': 59.63098968856431, 'wer_std': 29.609255000260532, 'wer_median': 57.14285714285714, 'wer_min': 0.0, 'wer_max': 1600.0, 'cer_mean': 20.513082126528033, 'cer_std': 20.685031205686414, 'cer_median': 16.0, 'cer_min': 0.0, 'cer_max': 2750.0, 'inference_time_sec_mean': 0.18741014499440212, 'inference_time_sec_std': 0.06796365556821761, 'inference_time_sec_median': 0.1851564683020115, 'inference_time_sec_min': 0.0, 'inference_time_sec_max': 1.2473665475845337, 'num_samples': 254490, 'model_name': 'omniASR_CTC_1B'}\n",
      "Metrics dataframe saved to /workspace/results/omniasr_inference/omniasr_ctc/omniASR_CTC_1B/metrics.df\n",
      "omniASR_CTC_300M\n",
      "{'wer_all': 64.72840744070258, 'cer_all': 23.660343479633273, 'wer_mean': 69.22924869355926, 'wer_std': 28.80134183342591, 'wer_median': 66.66666666666666, 'wer_min': 0.0, 'wer_max': 1400.0, 'cer_mean': 26.57213591647686, 'cer_std': 25.709417460134752, 'cer_median': 20.51282051282051, 'cer_min': 0.0, 'cer_max': 2700.0, 'inference_time_sec_mean': 0.07259715241698966, 'inference_time_sec_std': 0.04713245548070293, 'inference_time_sec_median': 0.08470884710550308, 'inference_time_sec_min': 0.0, 'inference_time_sec_max': 0.554960161447525, 'num_samples': 254490, 'model_name': 'omniASR_CTC_300M'}\n",
      "Metrics dataframe saved to /workspace/results/omniasr_inference/omniasr_ctc/omniASR_CTC_300M/metrics.df\n",
      "omniASR_CTC_3B\n",
      "{'wer_all': 51.95057683885393, 'cer_all': 17.56959840799964, 'wer_mean': 57.482973102436155, 'wer_std': 30.922365959158277, 'wer_median': 50.0, 'wer_min': 0.0, 'wer_max': 1700.0, 'cer_mean': 20.46093215079599, 'cer_std': 23.57906871706463, 'cer_median': 14.814814814814813, 'cer_min': 0.0, 'cer_max': 2800.0, 'inference_time_sec_mean': 0.45574874488950184, 'inference_time_sec_std': 0.1542291144214146, 'inference_time_sec_median': 0.4397813156247139, 'inference_time_sec_min': 0.0, 'inference_time_sec_max': 2.7982228696346283, 'num_samples': 254490, 'model_name': 'omniASR_CTC_3B'}\n",
      "Metrics dataframe saved to /workspace/results/omniasr_inference/omniasr_ctc/omniASR_CTC_3B/metrics.df\n",
      "omniASR_CTC_7B\n",
      "{'wer_all': 51.23880664760312, 'cer_all': 17.134426164274515, 'wer_mean': 56.60921750547232, 'wer_std': 30.62470749425906, 'wer_median': 50.0, 'wer_min': 0.0, 'wer_max': 1500.0, 'cer_mean': 19.957433767756772, 'cer_std': 23.730083940260954, 'cer_median': 14.285714285714285, 'cer_min': 0.0, 'cer_max': 2700.0, 'inference_time_sec_mean': 0.7453741821330231, 'inference_time_sec_std': 0.25982361704648377, 'inference_time_sec_median': 0.6905775368213654, 'inference_time_sec_min': 0.0, 'inference_time_sec_max': 5.924697279930115, 'num_samples': 254449, 'model_name': 'omniASR_CTC_7B'}\n",
      "Metrics dataframe saved to /workspace/results/omniasr_inference/omniasr_ctc/omniASR_CTC_7B/metrics.df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>wer_all</th>\n",
       "      <th>cer_all</th>\n",
       "      <th>wer_mean</th>\n",
       "      <th>wer_std</th>\n",
       "      <th>wer_median</th>\n",
       "      <th>wer_min</th>\n",
       "      <th>wer_max</th>\n",
       "      <th>cer_mean</th>\n",
       "      <th>cer_std</th>\n",
       "      <th>cer_median</th>\n",
       "      <th>cer_min</th>\n",
       "      <th>cer_max</th>\n",
       "      <th>inference_time_sec_mean</th>\n",
       "      <th>inference_time_sec_std</th>\n",
       "      <th>inference_time_sec_median</th>\n",
       "      <th>inference_time_sec_min</th>\n",
       "      <th>inference_time_sec_max</th>\n",
       "      <th>num_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>omniASR_CTC_7B</td>\n",
       "      <td>51.239</td>\n",
       "      <td>17.134</td>\n",
       "      <td>56.609</td>\n",
       "      <td>30.625</td>\n",
       "      <td>50.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>19.957</td>\n",
       "      <td>23.730</td>\n",
       "      <td>14.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.925</td>\n",
       "      <td>254449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omniASR_CTC_3B</td>\n",
       "      <td>51.951</td>\n",
       "      <td>17.570</td>\n",
       "      <td>57.483</td>\n",
       "      <td>30.922</td>\n",
       "      <td>50.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>20.461</td>\n",
       "      <td>23.579</td>\n",
       "      <td>14.815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.798</td>\n",
       "      <td>254490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omniASR_CTC_1B</td>\n",
       "      <td>54.652</td>\n",
       "      <td>18.164</td>\n",
       "      <td>59.631</td>\n",
       "      <td>29.609</td>\n",
       "      <td>57.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>20.513</td>\n",
       "      <td>20.685</td>\n",
       "      <td>16.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.247</td>\n",
       "      <td>254490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>omniASR_CTC_300M</td>\n",
       "      <td>64.728</td>\n",
       "      <td>23.660</td>\n",
       "      <td>69.229</td>\n",
       "      <td>28.801</td>\n",
       "      <td>66.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>26.572</td>\n",
       "      <td>25.709</td>\n",
       "      <td>20.513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.555</td>\n",
       "      <td>254490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model_name  wer_all  cer_all  wer_mean  wer_std  wer_median  wer_min  \\\n",
       "3    omniASR_CTC_7B   51.239   17.134    56.609   30.625      50.000      0.0   \n",
       "2    omniASR_CTC_3B   51.951   17.570    57.483   30.922      50.000      0.0   \n",
       "0    omniASR_CTC_1B   54.652   18.164    59.631   29.609      57.143      0.0   \n",
       "1  omniASR_CTC_300M   64.728   23.660    69.229   28.801      66.667      0.0   \n",
       "\n",
       "   wer_max  cer_mean  cer_std  cer_median  cer_min  cer_max  \\\n",
       "3   1500.0    19.957   23.730      14.286      0.0   2700.0   \n",
       "2   1700.0    20.461   23.579      14.815      0.0   2800.0   \n",
       "0   1600.0    20.513   20.685      16.000      0.0   2750.0   \n",
       "1   1400.0    26.572   25.709      20.513      0.0   2700.0   \n",
       "\n",
       "   inference_time_sec_mean  inference_time_sec_std  inference_time_sec_median  \\\n",
       "3                    0.745                   0.260                      0.691   \n",
       "2                    0.456                   0.154                      0.440   \n",
       "0                    0.187                   0.068                      0.185   \n",
       "1                    0.073                   0.047                      0.085   \n",
       "\n",
       "   inference_time_sec_min  inference_time_sec_max  num_samples  \n",
       "3                     0.0                   5.925       254449  \n",
       "2                     0.0                   2.798       254490  \n",
       "0                     0.0                   1.247       254490  \n",
       "1                     0.0                   0.555       254490  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.eval import fast_asr_metrics\n",
    "import json \n",
    "import pandas as pd\n",
    "from glob import glob \n",
    "# test_root_dir = \"/workspace/results/\"\n",
    "test_root_dir = \"/workspace/results/omniasr_inference/omniasr_ctc\"\n",
    "\n",
    "# test_root_dir ÎÇ¥ Î™®Îì† test_pred.parquet ÌååÏùºÏùÑ Ïû¨Í∑ÄÏ†ÅÏúºÎ°ú Ï∞æÍ∏∞\n",
    "test_files = sorted(glob(f\"{test_root_dir}/**/test_pred.parquet\", recursive=True))\n",
    "all_list = []\n",
    "for file_path in test_files:\n",
    "    save_path = file_path.replace(file_path.split(\"/\")[-1], \"metrics.df\") # file_path Ïùò ÎîîÎ†âÌÜ†Î¶¨\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    metrics = fast_asr_metrics(df, gt_col = \"gt_text\", pred_col = \"pred_text\")\n",
    "    metrics['model_name'] = file_path.split(\"/\")[-2]\n",
    "    print(file_path.split(\"/\")[-2])\n",
    "    print(metrics)\n",
    "\n",
    "    # metricsÎ•º Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏúºÎ°ú Î≥ÄÌôò ÌõÑ csvÎ°ú Ï†ÄÏû•\n",
    "    \n",
    "    df_metrics = pd.DataFrame([metrics])\n",
    "    df_metrics = df_metrics.sort_values(by='wer_mean', ascending=True)\n",
    "    df_metrics.to_csv(save_path, index=False)\n",
    "    print(f\"Metrics dataframe saved to {save_path}\")\n",
    "    \n",
    "    all_list.append(metrics)\n",
    "\n",
    "all_df = pd.DataFrame(all_list)\n",
    "# 'model_name' Ïª¨ÎüºÏù¥ Í∞ÄÏû• Ï≤´ Î≤àÏß∏ Ïª¨ÎüºÏúºÎ°ú Ïò§ÎèÑÎ°ù ÏàúÏÑú Ï°∞Ï†ï\n",
    "cols = ['model_name'] + [col for col in all_df.columns if col != 'model_name']\n",
    "all_df = all_df[cols]\n",
    "all_df = all_df.sort_values(by='wer_mean', ascending=True)\n",
    "all_df.to_csv(f\"{test_root_dir}/all_metrics.csv\", index=False)\n",
    "all_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def hh_mm_ss_to_seconds(time_str):\n",
    "    # Split off microseconds if present\n",
    "    if '.' in time_str:\n",
    "        time_str, micro = time_str.split('.')\n",
    "    else:\n",
    "        micro = '0'\n",
    "    t = datetime.datetime.strptime(time_str, \"%H:%M:%S\")\n",
    "    delta = datetime.timedelta(\n",
    "        hours=t.hour, minutes=t.minute, seconds=t.second, microseconds=int(micro)\n",
    "    )\n",
    "    return delta.total_seconds()\n",
    "\n",
    "def second_to_hh_mm_ss(seconds):\n",
    "    return datetime.timedelta(seconds=seconds)\n",
    "\n",
    "second = hh_mm_ss_to_seconds(\"10:49:38.484706\")\n",
    "print(second)\n",
    "\n",
    "hh_mm_ss = second_to_hh_mm_ss(second * 1 / 5)\n",
    "print(f\"{hh_mm_ss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54051133",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"/workspace/results/omniasr_inference/omniasr_ctc/omniASR_CTC_1B/test_pred.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
