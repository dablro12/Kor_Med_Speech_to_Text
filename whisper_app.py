import argparse
import os
import time
from typing import Optional, Tuple

import gradio as gr
import librosa
import numpy as np
import torch
import transformers
from transformers import WhisperForConditionalGeneration, WhisperProcessor, pipeline

os.environ["CUDA_VISIBLE_DEVICES"] = "0"
transformers.logging.set_verbosity_error()

# ì‹¤ì‹œê°„ ì„±ëŠ¥ ìœ ì§€ë¥¼ ìœ„í•œ ê³ ì • ì²˜ë¦¬ ìœˆë„ìš° í¬ê¸° (10ì´ˆ @ 16000 Hz)
# ì˜¤ë””ì˜¤ ë²„í¼ì˜ ë¬´í•œ ì¦ê°€ë¥¼ ë§‰ê³ , ë‚®ì€ ì§€ì—° ì‹œê°„ì„ ìœ ì§€í•˜ì—¬ ì‘ë‹µì„±ì„ ê°œì„ í•©ë‹ˆë‹¤.
WINDOW_SECONDS = 5
WINDOW_SAMPLES = 16000 * WINDOW_SECONDS

class WhisperRealtimeApp:
    def __init__(
        self,
        model_dir: str,
        sampling_rate: int = 16000,
        device: str = "cuda",
        language: str = "ko",
        task: str = "transcribe",
    ):
        self.sampling_rate = sampling_rate
        self.device = self._resolve_device(device)
        self.language = language
        self.task = task
        
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        model_name = os.path.basename(os.path.dirname(model_dir)) or "whisper-small"
        hf_model_id = f"openai/{model_name}"
        model_source = model_dir if os.path.exists(model_dir) else hf_model_id
        
        print(f"[INFO] Loading model from: {model_source}")

        # í”„ë¡œì„¸ì„œ ë° ëª¨ë¸ ë¡œë“œ
        self.processor = WhisperProcessor.from_pretrained(
            hf_model_id, language=language, task=task
        )
        self.model = WhisperForConditionalGeneration.from_pretrained(
            model_source
            # hf_model_id
        ).to(self.device)
        self.model.eval()
        
        # ê°•ì œ ë””ì½”ë”© ì„¤ì •
        forced_ids = self.processor.tokenizer.get_decoder_prompt_ids(
            language=language, task=task
        )
        if forced_ids is not None:
            self.model.generation_config.forced_decoder_ids = forced_ids

    @staticmethod
    def _resolve_device(device_str: str) -> torch.device:
        if device_str.startswith("cuda"):
            if torch.cuda.is_available():
                return torch.device(device_str)
            print("[WARN] CUDA unavailable. Falling back to CPU.")
        return torch.device("cpu")

    def transcribe(self, audio: np.ndarray) -> Tuple[str, float]:
        """
        16000Hz mono audio array -> text, latency
        """
        if len(audio) < self.sampling_rate * 0.5: # 0.5ì´ˆ ë¯¸ë§Œì€ ìŠ¤í‚µ
            return "", 0.0

        input_features = self.processor(
            audio, sampling_rate=self.sampling_rate, return_tensors="pt"
        ).input_features.to(self.device)

        if self.device.type == "cuda":
            torch.cuda.synchronize()
        start = time.time()
        
        with torch.no_grad():
            predicted_ids = self.model.generate(input_features)

        if self.device.type == "cuda":
            torch.cuda.synchronize()
        latency = time.time() - start
        
        transcription = self.processor.batch_decode(
            predicted_ids, skip_special_tokens=True
        )[0].strip()
        
        # Streaming ëª¨ë“œì—ì„œëŠ” latencyë¥¼ ë°˜í™˜í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ íŠœí”Œì˜ ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì‚¬ìš©
        return transcription, latency

def build_interface(app: WhisperRealtimeApp):
    # Gradio Blocksë¥¼ ì‚¬ìš©í•˜ì—¬ UI êµ¬ì„±
    with gr.Blocks(title="Real-time Whisper ASR (Streaming)") as demo:
        gr.Markdown(
            f"""
            # ðŸŽ™ï¸ Whisper ASR (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
            **1. ë§ˆì´í¬ë¡œ ë…¹ìŒ** ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì¸ì‹ì´ **ì‹¤ì‹œê°„**ìœ¼ë¡œ ì‹œìž‘ë©ë‹ˆë‹¤.
            **2. ë…¹ìŒ ì¤‘ì§€** ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ìŠ¤íŠ¸ë¦¼ì´ ì¢…ë£Œë©ë‹ˆë‹¤.
            
            **ì£¼ì˜**: ì‹¤ì‹œê°„ ì„±ëŠ¥ ìœ ì§€ë¥¼ ìœ„í•´ ëª¨ë¸ì€ **ìµœê·¼ {WINDOW_SECONDS}ì´ˆ**ì˜ ì˜¤ë””ì˜¤ë§Œ ì‚¬ìš©í•˜ì—¬ ì¸ì‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            """
        )
        
        # ìƒíƒœ ì €ìž¥ì„ ìœ„í•œ State (ëˆ„ì ëœ ì˜¤ë””ì˜¤)
        state = gr.State(None) 
        
        with gr.Row():
            # ìž…ë ¥: ë§ˆì´í¬ (Streaming)
            audio_input = gr.Audio(
                sources=["microphone"], 
                streaming=True, # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
                type="numpy",
                label="1. ë§ˆì´í¬ë¡œ ë…¹ìŒ (ì‹¤ì‹œê°„ ì²˜ë¦¬)",
            )
            
            # ì¶œë ¥: ì¸ì‹ëœ í…ìŠ¤íŠ¸
            text_output = gr.Textbox(
                label="2. ì‹¤ì‹œê°„ ì¸ì‹ ê²°ê³¼", 
                placeholder="ë§ì”€í•˜ì‹œë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ í…ìŠ¤íŠ¸ê°€ í‘œì‹œë©ë‹ˆë‹¤...",
                lines=10
            )
        
        # Streaming ì²˜ë¦¬ í•¨ìˆ˜
        def process_stream(stream: Optional[np.ndarray], new_chunk: Optional[Tuple[int, np.ndarray]]):
            if new_chunk is None:
                # ìƒˆë¡œìš´ ì²­í¬ê°€ ì—†ìœ¼ë©´ í˜„ìž¬ ìƒíƒœ ê·¸ëŒ€ë¡œ ë°˜í™˜
                return stream, ""
            
            sr, y = new_chunk
            
            # --- Preprocessing (Gradio input -> 16kHz np.ndarray) ---
            
            # 1. í¬ë§· ë³€í™˜ (float32)
            if y.dtype.kind == 'i':
                y = y.astype(np.float32) / 32768.0
            else:
                y = y.astype(np.float32)
                
            # 2. ëª¨ë…¸ ë³€í™˜
            if y.ndim > 1:
                y = y.mean(axis=1)
                
            # 3. ë¦¬ìƒ˜í”Œë§ (16kHz í•„ìˆ˜)
            if sr != app.sampling_rate:
                y = librosa.resample(y, orig_sr=sr, target_sr=app.sampling_rate)

            # 4. ìŠ¤íŠ¸ë¦¼ ëˆ„ì  (Tutorial logic: concatenate)
            if stream is not None:
                stream = np.concatenate([stream, y])
            else:
                stream = y
            
            # 5. ì‹¤ì‹œê°„ ì„±ëŠ¥ ìœ ì§€ë¥¼ ìœ„í•´, ëˆ„ì ëœ ì˜¤ë””ì˜¤ ì¤‘ ìµœê·¼ WINDOW_SECONDSë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            audio_segment_for_transcription = stream[-WINDOW_SAMPLES:]

            # 6. ì¶”ë¡  ì‹¤í–‰
            text, _ = app.transcribe(audio_segment_for_transcription) # latencyëŠ” ë¬´ì‹œ
            
            # 7. ìƒˆë¡œìš´ ëˆ„ì  ìŠ¤íŠ¸ë¦¼(ì „ì²´)ê³¼ í…ìŠ¤íŠ¸ ë°˜í™˜
            return stream, text

        def clear_state():
            # ë§ˆì´í¬ê°€ ë©ˆì¶”ê±°ë‚˜ ë¦¬ì…‹ë˜ì—ˆì„ ë•Œ ìƒíƒœ ì´ˆê¸°í™”
            return None, ""

        # ì´ë²¤íŠ¸ ì—°ê²°: ì˜¤ë””ì˜¤ ìž…ë ¥ì´ 'ìŠ¤íŠ¸ë¦¼'ë˜ë©´ í•¨ìˆ˜ ì‹¤í–‰
        audio_input.stream(
            process_stream,
            inputs=[state, audio_input],
            outputs=[state, text_output],
            show_progress=False # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” í”„ë¡œê·¸ë ˆìŠ¤ ë°” ìˆ¨ê¹€
        )
        
        # ì˜¤ë””ì˜¤ ì»´í¬ë„ŒíŠ¸ì˜ X ë²„íŠ¼ ë“±ì„ ëˆŒë €ì„ ë•Œ ìƒíƒœ ì´ˆê¸°í™”
        audio_input.clear(clear_state, outputs=[state, text_output])

    return demo

def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_dir", type=str, default="/workspace/results/whisper_train/whisper-tiny/checkpoint-15909")
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--sampling_rate", type=int, default=16000)
    parser.add_argument("--language", type=str, default="ko")
    parser.add_argument("--task", type=str, default="transcribe")
    parser.add_argument("--port", type=int, default=7860)
    return parser

def main():
    args = build_parser().parse_args()
    
    app = WhisperRealtimeApp(
        model_dir=args.model_dir,
        sampling_rate=args.sampling_rate,
        device=args.device,
        language=args.language,
        task=args.task
    )
    
    demo = build_interface(app)
    demo.launch(server_name="0.0.0.0", server_port=args.port, share=True)

if __name__ == "__main__":
    main()