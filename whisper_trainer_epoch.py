
import os
import sys
import time
from datetime import timedelta

os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["NCCL_P2P_DISABLE"] = "1"
os.environ["NCCL_IB_DISABLE"] = "1"

from transformers import (
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    WhisperForConditionalGeneration,
    WhisperProcessor,
)

sys.path.append("/workspace/notebook")
from utils.data_dataset import KruWhisperDataset, DataCollatorSpeechSeq2SeqWithPadding
from utils.metrics import compute_metrics

class WhisperTrainer:
    def __init__(
        self,
        model_id: str = "openai/whisper-small",
        language: str = "ko",
        task: str = "transcribe",
        train_csv_path: str = "/workspace/kru_data/train.csv",
        eval_csv_path: str = "/workspace/kru_data/test.csv",
        sampling_rate: int = 16000,
        output_dir: str = "/workspace/kru_data/results/whisper-small-ko",
        logging_dir : str = "/workspace/logs",
        do_eval: bool = False,  # í‰ê°€ ë¹„í™œì„± í”Œë˜ê·¸ ì¶”ê°€
    ) -> None:
        self.model_id = model_id
        self.language = language
        self.task = task
        self.train_csv_path = train_csv_path
        self.eval_csv_path = eval_csv_path
        self.sampling_rate = sampling_rate
        self.output_dir = output_dir
        self.logging_dir = logging_dir
        self.do_eval = do_eval  # í‰ê°€ í”Œë˜ê·¸ ì €ì¥

        self.processor = WhisperProcessor.from_pretrained(
            self.model_id,
            language=self.language,
            task=self.task,
        )
        self.feature_extractor = self.processor.feature_extractor
        self.tokenizer = self.processor.tokenizer

        self.train_dataset = self._build_dataset(self.train_csv_path)
        self.eval_dataset = self._build_dataset(self.eval_csv_path) if self.do_eval else None
        self.data_collator = DataCollatorSpeechSeq2SeqWithPadding(
            processor=self.processor,
            decoder_start_token_id=self.tokenizer.bos_token_id,
        )
        self.model = WhisperForConditionalGeneration.from_pretrained(self.model_id)
        self.training_args = self._build_training_args()
        self.trainer = self._build_trainer()

    def _build_dataset(self, csv_path: str) -> KruWhisperDataset:
        return KruWhisperDataset(
            csv_path=csv_path,
            feature_extractor=self.feature_extractor,
            tokenizer=self.tokenizer,
            sampling_rate=self.sampling_rate,
        )

    def _build_training_args(self) -> Seq2SeqTrainingArguments:
        return Seq2SeqTrainingArguments(
            output_dir=self.output_dir,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            gradient_accumulation_steps=1,
            learning_rate=1e-5,
            warmup_steps=100,

            # -----------------------
            # ğŸ”¥ Epoch ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½
            # -----------------------
            num_train_epochs=5,       # ì›í•˜ëŠ” epoch ìˆ˜ ì„¤ì •
            max_steps=-1,             # or None (Epoch ìš°ì„  ì ìš©)
            save_strategy="epoch",    # ë§¤ epoch ì €ì¥
            eval_strategy="no",       # í‰ê°€ ë¹„í™œì„±í™” (ê·¸ëŒ€ë¡œ)
            logging_strategy="steps", # ê·¸ëŒ€ë¡œ OK
            # -----------------------

            fp16=False,
            bf16=True,
            gradient_checkpointing=False,
            predict_with_generate=True,
            generation_max_length=225,
            logging_steps=25,

            report_to=["tensorboard"],
            load_best_model_at_end=False,
            metric_for_best_model="cer",
            greater_is_better=False,
            push_to_hub=False,
            logging_dir=self.logging_dir,
        )

    def _build_trainer(self) -> Seq2SeqTrainer:
        # í‰ê°€/metrics ê´€ë ¨ ì¸ìë„ ì¡°ê±´ì— ë”°ë¼ ë¹„í™œì„±í™”
        if self.do_eval:
            return Seq2SeqTrainer(
                args=self.training_args,
                model=self.model,
                train_dataset=self.train_dataset,
                eval_dataset=self.eval_dataset,
                data_collator=self.data_collator,
                compute_metrics=compute_metrics,
                tokenizer=self.processor.tokenizer,
            )
        else:
            return Seq2SeqTrainer(
                args=self.training_args,
                model=self.model,
                train_dataset=self.train_dataset,
                # eval_dataset=self.eval_dataset,  # í‰ê°€ ë°ì´í„°ì…‹ ë¹„í™œì„±í™”
                data_collator=self.data_collator,
                compute_metrics=compute_metrics,  # metrics ë¹„í™œì„±í™”
                tokenizer=self.processor.tokenizer,
            )

    def train(self) -> Seq2SeqTrainer:
        self.trainer.train()
        return self.trainer

def get_last_dir_name(path):
    """output_dirì—ì„œ ë§ˆì§€ë§‰ ë””ë ‰í† ë¦¬ ì´ë¦„ ì¶”ì¶œ"""
    return os.path.basename(os.path.normpath(path))

if __name__ == "__main__":
    # model_series = ['tiny', 'base', 'small', 'medium'] # 'large'ëŠ” OOM ë°œìƒ with RTX4090 
    model_series = ['tiny', 'base', 'small', 'medium']
    
    for model_name in model_series:
        whisper_trainer = WhisperTrainer(
                model_id=f"openai/whisper-{model_name}",
                language="ko",
                task="transcribe",
                train_csv_path="/workspace/kru_data/train.csv",
                eval_csv_path="/workspace/kru_data/test.csv",
                sampling_rate=16000,
                output_dir=f"/workspace/results/whisper_train/whisper-{model_name}",
                logging_dir=f"/workspace/logs/whisper-{model_name}", # tensorboard logging dir 
                do_eval=False,  # í‰ê°€ ë„ê¸°
            )
        
        # í›ˆë ¨ ì‹œì‘ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        whisper_trainer.train() # Training
        end_time = time.time()
        elapsed = end_time - start_time

        elapsed_td = timedelta(seconds=elapsed)
        elapsed_str = str(elapsed_td)
        # ë¡œê·¸ í´ë” ë§Œë“¤ê³  íŒŒì¼ ê²½ë¡œ ì§€ì •
        process_dir = whisper_trainer.output_dir + "/process"
        os.makedirs(process_dir, exist_ok=True)
        log_file_path = process_dir + "/time.log"

        with open(log_file_path, "a", encoding="utf-8") as f:
            f.write(f"Training time: {elapsed:.2f} seconds\n")
            f.write(f"Training time (hh:mm:ss): {elapsed_str}\n")

    print(f"Training time: {elapsed:.2f} seconds ({elapsed_str}) - saved to {log_file_path}")
